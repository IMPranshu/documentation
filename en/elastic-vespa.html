---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Elastic Vespa"
---

<p>
  A Vespa instance can grow and shrink while serving queries, reads and writes.
  To maintain a uniform data distribution due to changes in size or topology,
  documents are automatically redistributed, with minimal data movement.
  To resize, just change the <a href="reference/services-content.html#nodes">nodes</a>
  and redeploy the application - no restarts needed.
</p>
<img src="https://vespa.ai/assets/elastic-grow.svg" style="max-width: 300px;">
<p>
  The elasticity mechanism is also used to recover from a node loss -
  new replicas of documents are auto-created to maintain redundancy.
  Failed nodes is hence not a problem that requires immediate attention -
  as long as the instance has sufficient capacity to handle the data and load,
  it self-heals from node failures.
</p>
<img src="https://vespa.ai/assets/elastic-fail.svg" style="max-width: 300px;">
<p>
  Planned node retirements is done by marking nodes as <em>retired</em> -
  this generates replicas on the other nodes, so the set of retired nodes can safely be shut down.
  This makes it easy to migrate to a new node resource profile, like nodes with more memory.
</p>
<p>
  The auto-elasticity is configured for a normal fail-safe operation,
  but there are tradeoffs like recovery speed and resource usage.
  Learn more in <a href="operations/admin-procedures.html#content-cluster-configuration">procedures</a>.
</p>
<p>
  Document management is chunked into <a href="#buckets">buckets</a> with dynamic size and count.
  This means, there is no <em>shard</em> configuration in Vespa
  and managing a cluster of dynamic size and load is as easy as adding or removing nodes.
</p>


<h2 id="adding-nodes">Adding nodes</h2>
<p>
  Modify the configuration to add/remove nodes,
  then <a href="cloudconfig/application-packages.html#deploy">deploy</a>.
  Add an elastic content cluster to the application by adding a
  <a href="reference/services-content.html">content</a> element
  in <a href="reference/services.html">services.xml</a>.
  Read more in <a href="operations/admin-procedures.html">procedures</a>.
</p>
<p>
  When adding a new node, a new <em>ideal state</em> is calculated for all buckets.
  The buckets mapped to the new node are moved, the superfluous are removed.
  See redistribution example - add a new node to the system, with redundancy n=2:
</p>
<img src="content/img/AddNodeMoveBuckets.png"/>
<p>
  The distribution algorithm generates a random node sequence for each bucket.
  In this example with n=2, replicas map to the two nodes sorted first.
  The illustration shows how placement onto two nodes changes as a third node is added.
  The new node takes over as primary for the buckets where it got sorted first,
  and as secondary for the buckets where it got sorted second.
  This ensures minimal data movement when nodes come and go, and allows capacity to be changed easily.
</p><p>
  No buckets are moved between the existing nodes when a new node is added.
  Based on the pseudo-random sequences, some buckets change from primary to secondary, or are removed.
  Multiple nodes can be added in the same deployment.
  Adding more than one minimizes the total bucket redistribution,
  but increases the time to get to the <em>ideal state</em>.
</p>



<h2 id="removing-nodes">Removing nodes</h2>
<p>
  Whether a node fails or is <em>retired</em>, the same redistribution happens.
  If the node is retired, replicas are generated on the other nodes
  and the node stays up, but with no active replicas.
  Example of redistribution after node failure, n=2:
</p><img src="content/img/LoseNodeMoveBuckets.png"/><p>
  Here, node 2 fails. This node held the active replicas of bucket 2 and 6.
  Once the node fails the secondary replicas are set active.
  If they were already in a <em>ready</em> state, they start serving queries immediately,
  otherwise they will index replicas,
  see <a href="reference/services-content.html#searchable-copies">searchable-copies</a>.
  All buckets that no longer have secondary replicas
  are merged to the remaining nodes according to the ideal state.
</p>



<h2 id="grouped-distribution">Grouped distribution</h2>
<p>
  Use the <a href="reference/services-content.html#group">group</a> configuration
  to build a topology of the nodes.
  This enables replica placement for use cases listed below:
</p>
<img src="https://vespa.ai/assets/query-groups.svg" style="max-width: 400px;">
<table class="table">
  <thead></thead><tbody>
<tr>
  <th>Cluster upgrade</th>
  <td>
    Take out a full group for upgrade, instead of a node at a time.
    <a href="operations/live-upgrade.html">Read more</a>.
  </td>
</tr><tr>
  <th style="white-space: nowrap">Query throughput</th>
  <td>
    Applications with high query rates / high static query cost
    can confine queries to less nodes.
    <a href="performance/sizing-search.html">Read more</a>.
  </td>
</tr><tr>
  <th>Topology</th>
  <td>
    Place replicas based on network topology / racking of hosts
  </td>
</tr>
</tbody>
</table>
<p>
  Data redistribution after changing cluster topology is automatic,
  however note that query coverage is reduced in the redistribution period.
  Read more in <a href="performance/sizing-examples.html">sizing examples</a>.
</p><p>
  Tuning group sizes and node resources enables applications to easily find the latency/cost sweet spot,
  the elasticity operations are automatic and queries / reads / writes work as usual - no downtime!
</p>



<h2 id="buckets">Buckets</h2>
<p>
  To manage documents, Vespa groups them in <em>buckets</em>,
  using hashing or hints in the <a href="documents.html">document id</a>.
</p><p>
  A document Put or Update is sent to all replicas of the bucket with the document.
  If bucket replicas are out of sync, a bucket merge operation is run to re-sync the bucket.
  A bucket contains <a href="operations/admin-procedures.html#data-retention-vs-size">tombstones</a>
  of recently removed documents.
</p><p>
  Buckets are split when they grow too large, and joined when they shrink.
  This is a key feature for high performance in small to large instances,
  and eliminates need for downtime or manual operations when scaling.
  Read more in <a href="content/buckets.html">buckets</a>.
</p><p>
  Buckets can be viewed as sophisticated shards, and are not configured, they are dynamic.
  Query operations are run ignorant of buckets and makes query sizing independent of bucket distribution.
</p>
<p>
  Read more about <a href="content/buckets.html">buckets</a>.
</p>



<h2 id="ideal-state-distribution-algorithm">Ideal state distribution algorithm</h2>
<p>
  The <a href="content/idealstate.html">ideal state distribution algorithm</a>
  uses a variation of the <em>RUSH algorithms</em> to distribute documents.
  Under normal circumstances, it makes a minimal number of documents move when nodes are added or removed.
</p><p>
  Central to the algorithm is the assignment of a node sequence to each bucket.
  The illustration shows how each bucket uses a pseudo-random sequence of numbers to derive the node sequence.
  The pseudo-random generator is seeded with the bucket id, so each bucket will always get the same sequence:
</p>
<img src="/assets/img/bucket-node-sequence.svg" width="555px" height="auto"/>
<p>
  Each node has a <em>distribution-key</em>, which is an index in the number sequence.
  The set of number/index pairs is sorted on descending numbers, and this new sequence of indexes determines
  which nodes the replicas are placed on, with the first node being host to the primary replica, i.e. the <em>active</em> replica.
  This specification of where to place a bucket is called the bucket's <em>ideal state</em>.
</p>



<h2 id="consistency">Consistency</h2>
<p>
  Consistency is maintained at bucket level.
  Content nodes calculate checksums based on the bucket contents,
  and the content nodes compare checksums among the bucket replicas.
  A <em>bucket merge</em> is issued to resolve inconsistency, when detected.
  While there are inconsistent bucket replicas, operations are routed to the "best" replica.
</p><p>
  As buckets are split and joined, it is possible for replicas of a bucket to be split at different levels.
  A node may have been down while its buckets have been split or joined.
  This is called <em>inconsistent bucket splitting</em>.
  Bucket checksums can not be compared across buckets with different split levels.
  Consequently, content nodes do not know whether all documents exist in enough replicas in this state.
  Due to this, inconsistent splitting is one of the highest maintenance priorities.
  After all buckets are split or joined back to the same level,
  the content nodes can verify that all the replicas are consistent and fix any detected issues with a merge.
  <a href="content/consistency.html">Read more</a>.
</p>



<h2 id="further-reading">Further reading</h2>
<ul>
  <li><a href="content/content-nodes.html">content nodes</a></li>
  <li><a href="proton.html">proton</a> - see <em>ready</em> state</li>
</ul>
