---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Nearest Neighbor Search"
---
<p>
Searching for the nearest neighbors of a data point in high dimensional vector space is an important problem for many real time applications.
For example, in Computer Vision, searching for close data points in high dimensional vector space enables finding the most similar cats in large image datasets.
In Information Retrieval, large pre-trained natural language Transformer models like BERT, allows representing text sentences in dense embedding space,
where nearest neighbor search could serve as an effective multilingual neural retrieval function.
</p><p>
In many of these real world applications of nearest neighbor search, the search is constrained by real time query filters
applied over the data points metadata. For example, in E-Commerce search applications with constantly evolving metadata,
a search for nearest products for a query in vector space would typically be constrained by inventory status and price.
</p><p>
Expressing the search for nearest neighbors as a query operator allows combining the search for nearest neighbors with traditional query filters using boolean query operators.
</p>

<h2 id="using-vespas-nearest-neighbor-search">Using Vespa's nearest neighbor search</h2>
<p>The following sections demonstrates how to use the
<a href="reference/query-language-reference.html#nearestneighbor">nearestNeighbor</a>
query operator in Vespa.
The examples uses the brute force variant which has perfect accuracy
but which is computationally expensive for large document volumes as the distance needs to be calculated for every document which matches
the boolean query filters in the query. See how to speed up evaluation of nearest neighbor search by adding a <em>HNSW</em> index in
the <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search</a>.

<p>We demonstrate how to use the <em>nearestNeighbor</em>
query operator using a <a href="use-case-shopping.html">E-Commerce search use case</a>:

<ul>
  <li>Related products by product image similarity using SIFT image encodings</li>
  <li>Related products using text (title) embeddings similarity from a Transformer model (BERT)</li>
  <li>Related products using a hybrid combination of the above</li>
  <li>Hybrid Search using traditional IR term based matching in combination with neural vector space retrieval</li>
</ul>

We define the following product <a href="schemas.html">document schema</a>:
</p>

<pre>
schema product {

  document product {

    field title type string {
      indexing: summary | index
    }

    field in_stock type bool {
      indexing: summary | attribute
    }

    field price type float {
      indexing: summary | attribute
    }

    field popularity type float {
      indexing: summary | attribute
    }

    field image_sift_encoding type tensor&lt;float&gt;(x[128]) {
      indexing: summary | attribute
      attribute {
        distance-metric: euclidean
      }
    }

    field title_bert_embedding type tensor&lt;float&gt;(x[768]) {
      indexing: summary | attribute
      attribute {
        distance-metric: angular
      }
    }
  }
}
</pre>
<p>The <em>product</em> document type is made up of six fields.
Fields like <em>title</em>, <em>in_stock</em>, <em>price</em> and <em>popularity</em> are self-explanatory but the
<a href="tensor-user-guide.html">tensor</a> type fields needs more explanation.

<p>The <em>image_sift_encoding</em> field is used to store the
<a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">scale-invariant feature transform (SIFT)</a>
  encoding of a product catalog image.
The tensor definition specifies that it's indexed (dense) and has exactly 128 dimensions and uses float value precision.
When searching for nearest neighbors in the SIFT vector space we use
  <a href="reference/schema-reference.html#distance-metric"><em>euclidean</em></a>
 distance as our distance metric.</p>

<p>The <em>title_bert_embedding</em> field is used to store the
<a href="https://www.aclweb.org/anthology/D19-1410/">sentence-BERT embedding</a> which has 768 dimensions and we
use <a href="reference/schema-reference.html#distance-metric"><em>angular</em></a> as our distance metric.
There are different approaches
for using embeddings from large transformer natural language models like BERT for search, but that is out of scope for this document.
Please see the referenced paper for details on how to use BERT as a representation model where query and document text can be encoded independently.
</p>

<p>Vespa supports 4 distance metrics to use when searching for nearest neighbors:
<ul>
  <li><b>euclidean</b></li>
  <li><b>angular</b></li>
  <li><b>innerproduct</b></li>
  <li><b>geodegrees</b></li>
  <li><b>hamming</b></li>
</ul>

The first two are general distance metrics, while the last three may only be used for specific types of data.
See <a href="reference/schema-reference.html#distance-metric">distance-metric reference documentation</a> for details.

<p>We also need to define the types of the query tensors in the application package, see
  <a href="ranking-expressions-features.html#query-feature-types">query feature types</a> for details on query tensor feature types.  
</p>

<p>We need to add a new file in the application package <code>search/query-profiles/types/root.xml</code>. This 
file defines our query tensor inputs used by the nearest neighbor search operator. 
<pre>
&lt;query-profile-type id="root"&gt;
    &lt;tfield name="ranking.features.query(query_vector_sift)" type="tensor&lt;float&gt;(x[128])" /&gt;
    &lt;field name="ranking.features.query(query_vector_bert)" type="tensor&lt;float&gt;(x[768])" /&gt;
&lt/query-profile-type&gt;
</pre>

We also need to update <code>search/query-profiles/default.xml</code> to include these root types.
<pre>
&lt;query-profile id="default" type="root"&gt;
&lt/query-profile&gt;
</pre>

If you skip these steps you will face a query time error not recognizing the input tensor :
<pre>
Expected a tensor value of 'query(query_vector_sift)' but has [...]
</pre>

</p>

<p>Lastly, we need to configure how to <a href="ranking.html">rank</a> our products.
We start easy and define a rank profile which rank products by how similar the given query image is in the SIFT vector space:</p>

<pre>
  rank-profile image-similarity inherits default {
    first-phase {
      expression: closeness(field, image_sift_encoding)
    }
  }
</pre>

<p>We configured the tensor <em>distance-metric</em> as <em>euclidean</em>, and the <em>closeness(field, image_sift_encoding)</em>
Vespa rank feature which returns a value in the range [0, 1], where 0 is infinite distance,
and 1 is zero distance.</p>

<p>This is convenient because Vespa sorts documents by decreasing relevancy score and
we want the closest documents to be ranked highest.
This relevance score is decided by an expression configured in the ranking profile,
see the <a href="ranking.html">ranking</a> documentation.</p>

<p>The <em>first-phase</em> is part of Vespa's <a href="phased-ranking.html">phased ranking</a> support.
Phased ranking or multi-stage document ranking enables re-ranking of the top k subset of the hits which are retrieved from the previous stage.
Highly efficient query
operators like
<a href="reference/query-language-reference.html#nearestneighbor">nearestNeighbor</a>
or
<a href="using-wand-with-vespa.html">wand</a>
are effectively zero-phase retrieval as they are both top-k heap based and throws away hits which are
not making it into the top-K set. This pruning of low ranking hits in the zero-phase retrieval
reduces the number of hits which gets fully ranked using the <em>first-phase</em> function
and hence reduces latency and cost.</p>

<h3 id="indexing-product-data">Indexing product data</h3>
<p>
  Once we have deployed the document schema, one
  can <a href="reads-and-writes.html">index</a> the data using the
  <a href="reference/document-json-format.html">json feed format</a>.
  In the example below we feed two documents using the indexed tensors short form.
  Documents feed becomes visible in search immediately (real time indexing).
</p>
<pre>
[
  {
    "put": "id:shopping:product::998211",
    "fields": {
      "title": "Linen Blend Wide Leg Trousers",
      "in_stock": true,
      "price": 29.95,
      "popularity": 0.342,
      "image_sift_encoding": {
        "values": [
          0.9147281579191466,
          0.5453696694173961,
          0.7529545687063771,
          ..
         ]
      },
       "title_bert_embedding": {
        "values": [
          0.16766378547490635,
          0.3737005826272204,
          0.040492891373747675,
          ..
         ]
        }
      }
  },
  {
    "put": "id:shopping:product::97711",
    "fields": {
      "title": "Loose-fit White Pens",
      "in_stock": false,
      "price": 99.99,
      "popularity": 0.538,
      "image_sift_encoding": {
        "values": [
          0.9785931815169806,
          0.5697209315543527,
          0.5352198004501647,
          ..
        ]
      },
      "title_bert_embedding": {
        "values": [
          0.03515862084651311,
          0.24585168798559187,
          0.6123057708571111,
          ..
        ]
      }
    }
  }
]
</pre>
<p>
  The above json data can be fed to Vespa using any of the
  <a href="reads-and-writes.html#api-and-utilities">Vespa feeding api's</a>.
  Vespa only supports real time indexing so there is no explicit batch support,
  documents become searchable as they are fed to the system.
  Tensor fields can be <a href="reference/document-json-format.html#update">partially updated</a>
  as regular fields (without having to re-index the entire document).
</p>


<h3 id="querying-product-data">Querying product data</h3>
<p>Finally, we are able to start querying using the
<a href="reference/query-language-reference.html#nearestneighbor">nearestNeighbor</a> query operator.
This operator expects two arguments; the document tensor field we want to search and the query tensor we want to find
neighbors for. How many neighbors we want is controlled by the <em>targetHits</em> annotation which is a required parameter.
The query tensor is sent as a query ranking feature
and the query tensor name is referenced in second argument of the nearestNeighbor operator.</p>

<p>
<b>Related Products using Image Similarity</b></p>
<p>In this example we use the <em>nearestNeighbor</em> query operator to recommend similar products based on image similarity.
For a given image (e.g an product image shown in the product search result page) we can find
products which have a similar product image.
We can do this by fetching the SIFT encoding of the image we want to get similar images for
and use this tensor as input to the <em>nearestNeighbor</em> query operator and search over the collection of SIFT encodings.</p>

<p>
We are also not interested in the corpus wide global nearest neighbors but only products where <em>in_stock</em> is <em>true</em>.
We express the query logic using
the <a href="query-language.html">YQL query language</a> and our <a href="query-api.html#http">query api request</a> becomes:</p>

<pre>
{
  "yql": "select title, price from sources products where ([{\"targetHits\": 10}]nearestNeighbor(image_sift_encoding,query_vector_sift)) and in_stock = true;",
  "ranking.features.query(query_vector_sift)": [
    0.22507139604882176,
    0.11696498718517367,
    0.9418422036734729,
    ...
  ],
  "ranking.profile": "image-similarity",
  "hits": 10
}
</pre>

<p>The YQL express our query logic using logical conjunction <em>and</em> so that our nearest neighbor search is restricted to documents where <em>in_stock</em> is true.
The <em>targetHits</em> is the desired number of close neighbors we want.
The total number of hits which is ranked by the ranking profile
depends on the query filters and how fast the nearest neighbor search algorithm converges. The operator might expose more than 10 hits
to the <em>first-phase</em> ranking expression.  Since we are filtering on <em>in_stock</em>
one might also get 0 results if the entire product catalog is out of stock.

The <em>ranking.profile</em> parameter chooses the <em>rank-profile</em> we described earlier which simply ranks documents based on how similar they are in the SIFT
vector encoding space.
Finally, the <em>hits</em> parameter controls how many hits are
returned in the response. If the documents are partitioned over multiple content nodes the <em>targetHits</em> is per node, so with e.g 10 nodes we get at least
100 nearest neighbors fully evaluated using the <em>first-phase</em> ranking function.

In our example the <em>first-phase</em> expression is simply the <em>closeness</em> score as calculated by the <em>nearestNeighbor</em> operator
and hence the hits are not re-ordered but assigned the same score as calculated in the zero-phase by the <em>nearestNeighbor</em> query operator.

We could re-order hits by changing the first-phase ranking function, and the following would <em>re-rank</em> the nearest neighbors which are in stock using
the price:</p>

<pre>
rank-profile image-similarity-price {
  first-phase {
    expression: if(isNan(attribute(price)) == 1,0.0, attribute(price))
  }
}
</pre>
<p>In this example the ranking profile simply ignores the <em>closeness</em> score assigned by the <em>nearestNeighbor</em> operator
and instead re-rank the most similar images by their <em>price</em>, ranked in decreasing order.</p>

<p>
A more advanced example is given below where we retrieve the closest neighbors in SIFT-encoding vector space (with in stock filter)
and ranks those neighbors using a combination of the euclidean distance rank and the product popularity
(e.g. calculated offline based on past user behavior with the product) and finally re-ranks the top 100 hits per node
using a more advanced <a href="onnx.html">ONNX</a> imported Machine Learned (ML) model. These are examples of multi-stage or <a href="phased-ranking.html">phased ranking</a>.
</p>

<pre>
rank-profile image-similarity-with-onnx-stage {
  first-phase {
    expression: closeness(field, image_sift_encoding)*attribute(popularity)
  }
  second-phase {
    rerank-count: 100
    expression: onnx(image_similarity_model)
  }
 }
</pre>
<p>With the above multi-stage ranking profiles there are three stages:
<ul>
<li>The zero-phase nearest neighbor search which only rank documents by the closeness feature using <em>euclidean</em> distance</li>
<li>The <em>first-phase</em> which ranks the top <em>targetHits</em> from zero-phase</li>
<li>The <em>second-phase</em> which re-ranks the top 100 hits from <em>first-phase</em> ranking stage</li>
</ul>
<p>
<b>Related Products using BERT Text Embedding Similarity</b></p>
<p>As an alternative to using product image similarity to find related items we look at using the BERT embedding from the product title instead.
Similar to the image similarity case we assume that we
have the BERT embedding tensor representation for the product we want to find related or similar items for,
and can use this to search for similar products using the <em>angular</em> distance metric.  We define our
ranking function:</p>

<pre>
rank-profile title-similarity-using-bert {
  first-phase {
    expression: closeness(field, title_bert_embedding)
  }
}
</pre>
<p>
Searching for 10 nearest neighbors using <em>angular</em> distance in the BERT embedding vector space
and we are only interested in products where <em>in_stock</em> is true:
</p>
<pre>
{
  "yql": "select title,price from sources products where ([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert)) and in_stock = true;",
  "ranking.features.query(query_vector_bert)": [
    0.7528874147920314,
    0.712266471788029,
    0.8813024904379954,
    ...
  ],
  "ranking.profile": "title-similarity-using-bert",
  "hits": 10
}
</pre>
<p>
<b>Related Products using hybrid BERT Text Embedding Similarity</b></p>
<p>
We can also combine and retrieve products in a single query request using disjunction (OR)
logical operator to retrieve products which are close in either vector space and we define
our ranking function as a weighted linear combination of the <em>closeness</em> scores produced by two <em>nearestNeighbor</em> query operators:</p>

<pre>
rank-profile hybrid-similarity {
  first-phase {
    expression: query(bert_weight)*closeness(field, title_bert_embedding) + (1 - query(bert_weight))* closeness(field, image_sift_encoding)
  }
}
</pre>
<p>The search request need to pass both the BERT embedding tensor and the SIFT encoding tensor and we use logical disjunction (OR) to combine the
two <em>nearestNeighbor</em> query operators. Our search request becomes: </p>
<pre>
{
  "yql": "select title, price from sources products where (([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert))
or ([{\"targetNumHits\": 10}]nearestNeighbor(image_sift_encoding,query_vector_sift))) and in_stock = true;",
  "hits": 10,
  "ranking.features.query(query_vector_sift)": [
    0.5158057404746615,
    0.6441591144976925,
    0.24075880767944935,
    ...
  ],
  "ranking.features.query(query_vector_bert)": [
    0.708719978302217,
    0.4800850502044147,
    0.03310770782193717,
    ...
  ],
  "ranking.features.query(bert_weight)": 0.1,
  "ranking.profile": "hybrid-similarity"
}
</pre>
<p>
We ask for 10 nearest neighbors in the title BERT embedding vector space or 10 nearest neighbors in the SIFT encoding space
which in total will expose at least 20 documents to the
<em>first-phase</em> ranking function. </p>

<h3 id="hybrid-neural-term-retrieval">Hybrid Neural Term Retrieval</h3>
<p>In the following example we have a user input query (for example 'white trousers for woman') and we search for both products which matches
the query terms and for the nearest neighbors using the neural BERT embedding space.  In this case we need to produce the BERT embedding
of the input text query and we use logical disjunction. This allows also retrieving documents which does not use the exact same vocabulary as
in the user query. Our search request becomes: </p>
<pre>
{
  "yql": "select title, price from sources products where in_stock = true and (userQuery() or ([{\"targetNumHits\": 10}]nearestNeighbor(title_bert_embedding,query_vector_bert)));",
  "hits": 10,
  "ranking.features.query(query_vector_bert)": [
    0.10209943251234721,
    0.6693870055804121,
    0.5088424671764654,
    ...
  ],
  "ranking.profile": "hybrid-term-neural-retrieval",
  "query": "white pants for women",
  "type": "any"
}

</pre>
<p>The query will retrieve products based on the title BERT embedding similarity and term based matching. We use the following ranking profile:</p>
<pre>
  rank-profile hybrid-term-neural-retrieval {
    first-phase {
      expression: 0.5*fieldMatch(title) + 0.5*closeness(field, title_bert_embedding)
    }
  }
</pre>
<p>The retrieved hits are ranked using a linear combination of the <em>closeness</em>
and the <em>fieldMatch</em> rank features.
<a href="reference/string-segment-match.html">fieldMatch</a> is a native built in Vespa
text matching feature which also takes term proximity into account when computing the text ranking score.</p>

<h3 id="programatic-usage">Using Nearest Neighbor from a Searcher Component</h3>
<p>As with all query operators in Vespa, one can also build the query tree programmatically
in a custom <a href="searcher-development.html">Searcher component</a>.</p>
<p>In the example below we execute
the incoming query and fetch the tensor of the top ranking hit ranked by whatever rank-profile was specified in the search request.
We read the <em>image_sift_encoding</em> tensor from the top ranking hit and use it as the input for the <em>nearestNeighbor</em> query operator to find
related products (using the rank profile described in earlier sections).</p>

<pre>
package ai.vespa.example.searcher;

import com.yahoo.prelude.query.NearestNeighborItem;
import com.yahoo.search.Query;
import com.yahoo.search.Result;
import com.yahoo.search.Searcher;
import com.yahoo.search.result.Hit;
import com.yahoo.search.searchchain.Execution;
import com.yahoo.tensor.Tensor;

public class FeedbackSearcher extends Searcher {

    @Override
    public Result search(Query query, Execution execution) {
        Result result = execution.search(query);
        if (result.getTotalHitCount() == 0)
            return result;
        execution.fill(result); // fetch the summary data of the hits if needed
        Hit bestHit = result.hits().get(0);
        Tensor sift_encoding = (Tensor) bestHit.getField("image_sift_encoding");
        Query relatedQuery = new Query();
        relatedQuery.getRanking().setProfile("image-similarity");
        relatedQuery.getRanking().getFeatures().put("query(query_vector_sift)", sift_encoding);
        NearestNeighborItem nn = new NearestNeighborItem("image_sift_encoding",  "query_vector_sift");
        nn.setTargetNumHits(10);
        relatedQuery.getModel().getQueryTree().setRoot(nn);
        return execution.search(relatedQuery);
    }
}
</pre>
